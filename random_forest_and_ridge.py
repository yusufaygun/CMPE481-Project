# -*- coding: utf-8 -*-
"""481-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rtx2TAO0b7Lmtjc5Qp0VFxVIKqkxrahD

# A. Initialization and Data Data Analysis
"""

!pip install kaggle
!pip install pandas numpy matplotlib seaborn scikit-learn

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

# Kaggle API
from google.colab import files
files.upload()


!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

#  dataset
!kaggle datasets download -d uciml/red-wine-quality-cortez-et-al-2009
!unzip red-wine-quality-cortez-et-al-2009.zip

df = pd.read_csv('winequality-red.csv')
print("Dataset loaded successfully. First five rows:")
print(df.head())

"""## 1) PCA"""

plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""## 2) Individual Feature Correlations with Quality (Outcome)"""

# feature correlation with the outcome without PCA
correlations = df.corr()['quality'].sort_values(ascending=False)
print("\nIndividual Feature Correlations with Quality (Outcome):")
print(correlations)

plt.figure(figsize=(10, 6))
sns.barplot(x=correlations.values, y=correlations.index, palette="coolwarm")
plt.title('Correlation of Features with Quality')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Features')
plt.show()

X=df.drop('quality',axis=1)
y= df['quality']


#standardizingfeatures
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA
pca_full = PCA()
X_pca_full=pca_full.fit_transform(X_scaled)

# explained variance ratio
explained_variance_df = pd.DataFrame({
    'Principal Component': [f'PC{i+1}' for i in range(len(pca_full.explained_variance_ratio_))],
    'Explained Variance Ratio': pca_full.explained_variance_ratio_,
    'Cumulative Variance': np.cumsum(pca_full.explained_variance_ratio_)
})
print("\nExplained Variance Ratio:")
display(explained_variance_df)

"""## 3) Explained Variance Ratio"""

from matplotlib import pyplot as plt
explained_variance_df['Explained Variance Ratio'].plot(kind='hist', bins=20, title='Explained Variance Ratio')
plt.gca().spines[['top', 'right',]].set_visible(False)

# PCA loadings
loadings = pd.DataFrame(
    pca_full.components_.T,
    columns=[f'PC{i+1}' for i in range(len(pca_full.explained_variance_ratio_))],
    index=X.columns
)
print("\n PCA Loadings:")
display(loadings)



#scree plot
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(pca_full.explained_variance_ratio_)+1), pca_full.explained_variance_ratio_, marker='o', linestyle='--')
plt.title('Scree Plot')
plt.xlabel('Principal Component ')
plt.ylabel('Explained Variance Ratio')
plt.grid(True)
plt.show()

# PCA retaining % 95 of the variance
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)
print(f'\n # principal components retained: {pca.n_components_}')

import pandas as pd
from sklearn.decomposition import PCA
import numpy as np

X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
# PCA
pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled_df)

print(f'# Principal components retained: {pca.n_components_}')


components = pd.DataFrame(
    pca.components_,
    columns=X_scaled_df.columns,  # Use actual feature names
    index=[f'Principal Component {i+1}' for i in range(pca.n_components_)]
)

print('\n# Contributions of each original feature to principal components:')
print(components)


feature_contributions = np.sum(np.abs(pca.components_), axis=0)
sorted_indices = np.argsort(-feature_contributions)  # Sort in descending order of importance

# separate retained and not retained features
retained_features = [X_scaled_df.columns[i] for i in sorted_indices[:pca.n_components_]]
not_retained_features = [X_scaled_df.columns[i] for i in sorted_indices[pca.n_components_:]]

print('\n# Retained features based on contributions:')
print(retained_features)

print('\n# Not retained features based on contributions:')
print(not_retained_features)

# splitting data without PCA
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42)

# splitting data with PCA
X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(
    X_pca, y, test_size=0.2, random_state=42)

# calculate metrics
def calculate_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mae, mse, r2

## 2) Random Forest

#1 With PCA & GridSearch

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['auto', 'sqrt']
}

grid_search_pca = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=2
)
grid_search_pca.fit(X_train_pca, y_train_pca)
best_params_pca = grid_search_pca.best_params_
print(f"\nBest Parameters With PCA: {best_params_pca}")

optimized_rf_pca = RandomForestRegressor(**best_params_pca, random_state=42)
optimized_rf_pca.fit(X_train_pca, y_train_pca)
y_pred_pca_grid = optimized_rf_pca.predict(X_test_pca)
metrics_pca_grid = calculate_metrics(y_test_pca, y_pred_pca_grid)

#2 With PCA & Without GridSearch
rf_pca_simple = RandomForestRegressor(random_state=42)
rf_pca_simple.fit(X_train_pca, y_train_pca)
y_pred_pca_simple = rf_pca_simple.predict(X_test_pca)
metrics_pca_simple = calculate_metrics(y_test_pca, y_pred_pca_simple)

#3 Without PCA & GridSearch

grid_search_orig = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=2
)
grid_search_orig.fit(X_train_orig, y_train_orig)
best_params_orig = grid_search_orig.best_params_
print(f"\nBest Parameters Without PCA: {best_params_orig}")

optimized_rf_orig = RandomForestRegressor(**best_params_orig, random_state=42)
optimized_rf_orig.fit(X_train_orig, y_train_orig)
y_pred_orig_grid = optimized_rf_orig.predict(X_test_orig)
metrics_orig_grid = calculate_metrics(y_test_orig, y_pred_orig_grid)

#4 Without PCA & Without GridSearch

rf_orig_simple = RandomForestRegressor(random_state=42)
rf_orig_simple.fit(X_train_orig, y_train_orig)
y_pred_orig_simple = rf_orig_simple.predict(X_test_orig)
metrics_orig_simple = calculate_metrics(y_test_orig, y_pred_orig_simple)

comparison_df = pd.DataFrame({
    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R² Score'],
    'With PCA & GridSearch': metrics_pca_grid,
    'With PCA & Without GridSearch': metrics_pca_simple,
    'Without PCA & GridSearch': metrics_orig_grid,
    'Without PCA & Without GridSearch': metrics_orig_simple
})
print("\nComparison of Metrics Across All Cases:")
display(comparison_df)

#residuals for all cases
residuals_without_pca = y_test_orig - y_pred_orig_grid
residuals_with_pca = y_test_pca - y_pred_pca_grid

plt.figure(figsize=(10, 6))
sns.histplot(residuals_without_pca, bins=30, kde=True, color='blue', label='Without PCA')
sns.histplot(residuals_with_pca, bins=30, kde=True, color='green', label='With PCA')
plt.title('Residual Distribution for Models')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.axvline(0, color='red', linestyle='--', label='Zero Residual Line')
plt.legend(loc='best')
plt.tight_layout()
plt.show()

# plot for comparison
comparison_df_melted = comparison_df.melt(id_vars='Metric', var_name='Case', value_name='Value')
plt.figure(figsize=(12, 8))
sns.barplot(x='Metric', y='Value', hue='Case', data=comparison_df_melted, palette="viridis")
plt.title('Comparison of Metrics Across Cases')
plt.ylabel('Metric Value')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()



## 4) Ridge Regression

# apply ridge regression without PCA
ridge_without_pca = Ridge(alpha=1.0, random_state=42)
ridge_without_pca.fit(X_train_orig, y_train_orig)
y_pred_without_pca = ridge_without_pca.predict(X_test_orig)

# apply ridge regression  with PCA
ridge_with_pca = Ridge(alpha=1.0, random_state=42)
ridge_with_pca.fit(X_train_pca, y_train_pca)
y_pred_with_pca = ridge_with_pca.predict(X_test_pca)

# errors before optimization
mae_without_pca = mean_absolute_error(y_test_orig, y_pred_without_pca)
mse_without_pca = mean_squared_error(y_test_orig, y_pred_without_pca)
r2_without_pca = r2_score(y_test_orig, y_pred_without_pca)

mae_with_pca = mean_absolute_error(y_test_pca, y_pred_with_pca)
mse_with_pca = mean_squared_error(y_test_pca, y_pred_with_pca)
r2_with_pca = r2_score(y_test_pca, y_pred_with_pca)

print("\n errors without PCA (before optimization):")
print(f"Mean Absolute Error: {mae_without_pca:.2f}")
print(f"Mean Squared Error: {mse_without_pca:.2f}")
print(f"R² Score: {r2_without_pca:.2f}")

print("\n errors with PCA (before optimization):")
print(f"Mean Absolute Error: {mae_with_pca:.2f}")
print(f"Mean Squared Error: {mse_with_pca:.2f}")
print(f"R² Score: {r2_with_pca:.2f}")

#hyperparameter  tuning

# parameter grid for alpha
alpha_range = {'alpha': np.logspace(-3, 3, 50)}  # Test alpha values from 0.001 to 1000

#GridSearchCV
ridge_cv_without_pca = GridSearchCV(
    estimator=Ridge(random_state=42),
    param_grid=alpha_range,
    scoring='r2',
    cv=5,
    n_jobs=-1
)
ridge_cv_with_pca = GridSearchCV(
    estimator=Ridge(random_state=42),
    param_grid=alpha_range,
    scoring='r2',
    cv=5,
    n_jobs=-1
)

# fitt GridSearchCV for both models
ridge_cv_without_pca.fit(X_train_orig, y_train_orig)
ridge_cv_with_pca.fit(X_train_pca, y_train_pca)

#best alpha values
best_alpha_without_pca=ridge_cv_without_pca.best_params_['alpha']
best_alpha_with_pca=ridge_cv_with_pca.best_params_['alpha']
print(f"\nBest Alpha Without PCA: {best_alpha_without_pca}")
print(f"Best Alpha With PCA: {best_alpha_with_pca}")

#ridge  with best alpha values
optimized_ridge_without_pca = Ridge(alpha=best_alpha_without_pca, random_state=42)
optimized_ridge_with_pca = Ridge(alpha=best_alpha_with_pca, random_state=42)

optimized_ridge_without_pca.fit(X_train_orig, y_train_orig)
optimized_ridge_with_pca.fit(X_train_pca, y_train_pca)

# predictions
y_pred_optimized_without_pca = optimized_ridge_without_pca.predict(X_test_orig)
y_pred_optimized_with_pca = optimized_ridge_with_pca.predict(X_test_pca)

# errors for optimized ridge regression
mae_optimized_without_pca = mean_absolute_error(y_test_orig, y_pred_optimized_without_pca)
mse_optimized_without_pca = mean_squared_error(y_test_orig, y_pred_optimized_without_pca)
r2_optimized_without_pca = r2_score(y_test_orig, y_pred_optimized_without_pca)

mae_optimized_with_pca = mean_absolute_error(y_test_pca, y_pred_optimized_with_pca)
mse_optimized_with_pca = mean_squared_error(y_test_pca, y_pred_optimized_with_pca)
r2_optimized_with_pca = r2_score(y_test_pca, y_pred_optimized_with_pca)

print("\nOptimized Ridge Regression Without PCA:")
print(f"Mean Absolute Error: {mae_optimized_without_pca:.2f}")
print(f"Mean Squared Error: {mse_optimized_without_pca:.2f}")
print(f"R² Score: {r2_optimized_without_pca:.2f}")

print("\nOptimized Ridge Regression With PCA:")
print(f"Mean Absolute Error: {mae_optimized_with_pca:.2f}")
print(f"Mean Squared Error: {mse_optimized_with_pca:.2f}")
print(f"R² Score: {r2_optimized_with_pca:.2f}")

comparison_df = pd.DataFrame({
    'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'R² Score'],
    'Without PCA (Before Optimization)': [mae_without_pca, mse_without_pca, r2_without_pca],
    'Without PCA (After Optimization)': [mae_optimized_without_pca, mse_optimized_without_pca, r2_optimized_without_pca],
    'With PCA (Before Optimization)': [mae_with_pca, mse_with_pca, r2_with_pca],
    'With PCA (After Optimization)': [mae_optimized_with_pca, mse_optimized_with_pca, r2_optimized_with_pca]
})
print("\nComparison of Metrics Before and After Optimization:")
display(comparison_df)

comparison_df_melted = comparison_df.melt(id_vars='Metric', var_name='Model', value_name='Value')
plt.figure(figsize=(12, 8))
sns.barplot(x='Metric', y='Value', hue='Model', data=comparison_df_melted, palette="viridis")
plt.title('Comparison of Metrics Before and After Optimization')
plt.ylabel('Metric Value')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# cross-val
cv_scores_without_pca = cross_val_score(optimized_ridge_without_pca, X_scaled, y, scoring='r2', cv=5)
cv_scores_with_pca = cross_val_score(optimized_ridge_with_pca, X_pca, y, scoring='r2', cv=5)

print("\nCross-Validation R² Scores Without PCA:")
print(cv_scores_without_pca)
print(f"Mean Cross-Validation R² Score Without PCA: {np.mean(cv_scores_without_pca):.2f}")

print("\nCross-Validation R² Scores With PCA:")
print(cv_scores_with_pca)
print(f"Mean Cross-Validation R² Score With PCA: {np.mean(cv_scores_with_pca):.2f}")

